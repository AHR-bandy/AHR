# AHR
## Application of Handwriting Recognition

### 画像認識の基本的な考え方（白黒文字）
 1. 全ての画像ファイルで縦横を分割する（準備過程）
 1. 分割して生成されたブロック群の中で各セルに学習データで同傾向の多さを基準に重みを付ける（学習過程）
 1. 重みを基に最小二乗法等によって検査する文字がどの文字にあたるかのもっともらしさを求め、最も値の高い文字を出力する（テスト過程）

### 教師なし学習で用いる最尤法（非線形回帰分析）
>* 最尤法では、被説明変数に０または１を取らせるために説明変数がどのような値を取っても０から１の範囲内に収まる式を用いる
>* これには線形回帰分析での関数:Yi=α+βXi を変形したロジスティック関数:Yi=exp(x)/(1+exp(x)) ※x=α+βXi　を用いる

### わかりやすいパターン認識1章
>* パターン認識には前処理、特徴抽出、識別演算の三段階の処理がある。
>* 前処理はノイズ削除の段階、特徴抽出は量子化と標本化の段階、識別演算はモデルと照合して文字を出力する段階
>* *量子化：濃度レベルを制限して有限にする（白黒の文字認識の場合なし）
>* *標本化：画像をブロックで区切り、ブロック内の色を統一


### 学習法には以下の種類がある <br>
#### ノンパラメトリックな学習
<br>   確率で発生するパラメーターを持たず、学習パターンから直接識別関数を求める方法
>* パーセプトロン（入力と出力の２層で構成される）
<br>   ―　線形識別関数（位置情報を持つ散布図に直線を引き、分割する方法）
<br>   ―　区分的線形識別関数（同散布図に折れ線を引き、分割する方法）
<br>   ―　Widrow-Hoffの学習規則（しきい値関数を用いて重みを修正し、分割する位置を模索する方法）
>* ニューラルネットワーク（入力と多数の中間層と出力の多層で構成される）
<br>   ―　誤差逆伝播法（微分可能なジグモイド関数を用いて前の層の重みを修正する）
#### パラメトリックな学習
<br>   学習パターンから確率密度関数のパラメーター推定を行い、そのパラメーターを真の値として識別関数を求める方法
<br>   （確率密度関数：正規分布などの曲線を描く式で、出力の合計が１になる）


### 汎化能力判別法
<br>   汎化能力（未知のデータに対する識別能力）を評価するにあたって、有限個のデータから偏りのなくテストデータと学習用データを取り出すために以下のような方法が取られる。
>* ホールドアウト法
<br>   手元データをテストデータと学習用データに分割して行う方法。
<br>   テストデータを増やせば学習用データが減る。
>* 交差確認法
<br>   手元のデータを複数個のグループに分割して、内一つをテストデータ、残りを学習データとして学習する。
<br>   これをすべてのデータグループがテストデータとなるまで行う方法。
<br>   分割によってデータに偏りが生じる可能性があるため、分割方法を変えてその誤識別率の平均などから判断する必要がある。
>* 一つ抜き法
<br>   グループ単位でなくデータ単位での交差確認法
<br>   この場合組み合わせ方は１種類なので、１回行えばよい。
>* ブートストラップ法
<br>   学習データをテストデータとしてもう一度使った時の誤認識率である再代入誤り率のバイアスを補正するために利用される。
<br>   N個データがあるとして、N回復元抽出を行って作ったブートストラップサンプルを用いて修正する。
<br>   １．ブートストラップサンプルを抽出、２．元データの再代入誤り率からブートストラップサンプルの再代入誤り率を引き、３．ブートストラップサンプルを学習データ、元データをテストデータとした誤り率を加える３過程の作業を複数回繰り返した（最低でも５０回）ものを誤識別率の予測値と判断する。

### 線形識別関数　－　線形判別関数
<br>   （自分の認識では、）教師なし学習の一種。
<br>   各クラスの平均同士の距離が大きく、クラスの分散が小さい状態で、線形の境界を見つける手法
<br>   各クラスの平均が離れていればそれだけ同横軸上で重なるデータは少なくなる
<br>   しかし、分散が大きければそれだけ横の広がりが大きいため、重なるデータは多くなるため、平均の距離の最大化と分散の最小化を同時に達成する必要がある
<br>   具体的にはYi=α+βXiを仮定し、Yiの平均をμ、分散をσ^2として(μi-μj)^2/(σi^2+σj^2)を最大とするβを見つける
>* 線形識別関数の教師あり学習に分類される、最小２乗誤差
<br>   教師入力と理論上の教師入力の誤差を０にする＝係数（αやβ）を操作して１次元上での境界を識別するもの
<br>   統計学の手法である最小２乗法を用いた学習方法
<br>   ｛観測値（Yi）を説明変数（Xi）の関数として、Yi=α+βXi+ε　※ε：誤差項　とし、
<br>   　平均では誤差項は０と仮定した上でαとβを導出、
<br>   　誤差項を除いた上式に代入して出た理論値（y）との誤差を０とする｝
>* 線形判別関数のメリット
<br>   次元が大きく、データの次元＞識別対象とするクラス数の時、真価を発揮する。
<br>   行列の写像によりXiのベクトル内のデータ数であるデータの次元からクラス数へ次元を下げる処理をする際に最大平均距離かつ最小分散となるようなβを見つけるため、誤認識を減らせる。

### 夏休暇中の課題
<br>   サンプルデータを識別してみる
